# 模型支持的列表以及下载地址
后处理是直接扣的 [ax-samples](https://github.com/AXERA-TECH/ax-samples)。

### 备注
- FPS 仅表示 [sample_vin_ivps_joint_vo](../examples/sample_vin_ivps_joint_vo) 在当前状态测得的速度，无法代表其他应用在其他场景及模型测得的速度
- OSD 对 CPU 资源的占用不容小觑，在某些模型推理时可能会极大的影响模型的推理速度以及整个pipeline的运行效率，其中带有 mask 信息的模型尤为明显
- 某些多级模型的配置文件是可以组合使用的，例如 ax_pose 既可以与 ax_person_det.joint组合使用，也可以与 yolov5s 组合使用，同理，hrnet 也可以与 ax_person_det.joint 组合使用，如 [ax_pose_yolov5s.json](../examples/libaxdl/config/ax_pose_yolov5s.json) 和 [hrnet_pose_ax_det.json](../examples/libaxdl/config/hrnet_pose_ax_det.json)

|模型|FPS|[枚举值](../examples/sample_run_joint/sample_run_joint_post_process.h)|下载地址|配置文件(有则表示主线已经支持)|备注|
|-|-|-|-|-|-|
|yolov5|25|```MT_DET_YOLOV5```|[bgr](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov5s.joint) / [nv12](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov5s-face.joint)|[yolov5s.json](../examples/libaxdl/config/yolov5s.json)|[如何更换自己训练的 yolov5 模型](../docs/how_to_deploy_custom_yolov5_model.md)|
|yolov5-face|30|```MT_DET_YOLOV5_FACE```|[bgr](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov5s-face.joint) / [nv12](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov5s_face_nv12_11.joint)|[yolov5s_face.json](../examples/libaxdl/config/yolov5s_face.json)|[yolov5-face](https://github.com/deepcam-cn/yolov5-face)|
|yolov5-seg|15|```MT_INSEG_YOLOV5_MASK```|[bgr](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov5s-seg.joint)|[yolov5_seg.json](../examples/libaxdl/config/yolov5_seg.json)|~~有内存泄漏，cv::Mat指针无法delete，但是可以release，泄露几个小时演示一下是没问题的~~ ***修好了***|
|yolov7|43|```MT_DET_YOLOV7```|[bgr](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov7-tiny.joint)|[yolov7.json](../examples/libaxdl/config/yolov7.json)|-|
|yolox|21|```MT_DET_YOLOX```|[bgr](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolox_s.joint)|[yolox.json](../examples/libaxdl/config/yolox.json)|-|
|nanodet|17|```MT_DET_NANODET```|[bgr](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/nanom.joint)|[nanodet.json](../examples/libaxdl/config/nanodet.json)|需要改很多东西，模型转换可能比较困难|
|pp-human-seg|30|```MT_SEG_PPHUMSEG```|[bgr](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/pp_human_seg_mobile_sim.joint) / [nv12](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/pp_human_seg_mobile_sim_nv12.joint)|[pp_human_seg.json](../examples/libaxdl/config/pp_human_seg.json)|~~在 [fork的分支](https://github.com/ZHEQIUSHUI/ax-pipeline/tree/pphumseg) 实现了，但是只在```sample_vin_ivps_joint_vo```实现了，暂时不好合到主线，可能是以后的kpi，未来可期~~ ***主线已支持***|
|hrnet-human-pose|11|```MT_MLM_HUMAN_POSE_HRNET```|[yolov5s](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov5s.joint) [hrnet](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/hrnet_256x192.joint)|[hrnet_pose.json](../examples/libaxdl/config/hrnet_pose.json)|前置通过yolov5s检测，抠图进行人体姿态检测。曲线救国解决了此bug~~暂时有bug，已经push同事在帮忙修，将就先用。允许两个模型不同格式串联使用两个模型暂时必须为相同的 NV12/BGR/RGB 格式输入~~。|
|ax-person-det|43|```MT_DET_YOLOX_PPL```|[ax_person_det](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/ax_person_det.joint)|[ax_person_det.json](../examples/libaxdl/config/ax_person_det.json) |爱芯元智算法组训练的yolox人体检测模型开源版本|
|ax-human-pose|25|```MT_MLM_HUMAN_POSE_AXPPL```|[ax_person_det](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/ax_person_det.joint) [ax_pose](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/ax_pose.joint)|[ax_pose.json](../examples/libaxdl/config/ax_pose.json)|爱芯元智算法组训练的人体姿态开源版本，前置通过```ax-person-detection```检测人体，抠图进行人体姿态检测。允许两个模型不同格式串联使用~~两个模型暂时必须为相同的 NV12/BGR/RGB 格式输入~~。|
|palm-hand-detection|40|```MT_DET_PALM_HAND```|[palm_detection](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/palm_detection.joint)|[palm_hand_detection.json](../examples/libaxdl/config/palm_hand_detection.json)|mediepipe的人手检测模型，感谢 [FeiGeChuanShu](https://github.com/FeiGeChuanShu) 适配到爱芯派平台|
|hand-pose|22|```MT_MLM_HAND_POSE```|[palm_detection](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/palm_detection.joint) [handpose](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/handpose.joint)|[hand_pose.json](../examples/libaxdl/config/hand_pose.json)|mediepipe的人手姿态模型，感谢 [FeiGeChuanShu](https://github.com/FeiGeChuanShu) 适配到爱芯派平台|
|yolopv2|-|```MT_DET_YOLOPV2```|再等等|[yolopv2.json](../examples/libaxdl/config/yolopv2.json)|感谢 [FeiGeChuanShu](https://github.com/FeiGeChuanShu) 适配到爱芯派平台|
|yolo-fastbody|43|```MT_DET_YOLO_FASTBODY```|[yolo-fastbody](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolo-fastbody.joint)|[yolo_fastbody.json](../examples/libaxdl/config/yolo_fastbody.json) |-|
|license-plate-detect|25|```MT_DET_LICENSE_PLATE```|[license_plate_det](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/license_plate_det.joint)|[yolov5s_license_plate.json](../examples/libaxdl/config/yolov5s_license_plate.json) |[License-Plate-Detector](https://github.com/zeusees/License-Plate-Detector.git)|
|license-plate-recognition|20|```MT_MLM_VEHICLE_LICENSE_RECOGNITION```|[license_plate_det](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/license_plate_det.joint) [license_plate_rec](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/license_plate_rec.joint)|[license_plate_recognition.json](../examples/libaxdl/config/license_plate_recognition.json)|[License-Plate-Detector](https://github.com/zeusees/License-Plate-Detector.git) [crnn_plate_recognition](https://github.com/we0091234/crnn_plate_recognition)|
|yolov7-face|25|```MT_DET_YOLOV7_FACE```|[yolov7-face](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov7-face.joint)|[yolov7_face.json](../examples/libaxdl/config/yolov7_face.json) |-|
|yolov7-plam-hand|25|```MT_DET_YOLOV7_PALM_HAND```|[yolov7-palm-hand](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov7-palm-hand.joint)|[yolov7_palm_hand.json](../examples/libaxdl/config/yolov7_palm_hand.json) |-|
|yolov6|-|```MT_DET_YOLOV6```|[yolov6s](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov6s.joint)|[yolov6.json](../examples/libaxdl/config/yolov6.json) |-|
|face-recognition|-|```MT_MLM_FACE_RECOGNITION```|[yolov5s-face](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov5s-face.joint) [w600k_mbf](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/w600k_mbf.joint)|[yolov5s_face_recognition.json](../examples/libaxdl/config/yolov5s_face_recognition.json) |子模型的FACE_DATABASE节点里面，名字作为key，图片路径作为value，人脸识别模型来自 [insightface](https://github.com/deepinsight/insightface/tree/master/model_zoo#list-of-models-by-mobilefacenet-and-different-training-datasets)|
|scrfd|26|```MT_DET_SCRFD```|[scrfd_500m_bnkps_shape640x640](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/scrfd_500m_bnkps_shape640x640.joint)|[scrfd.json](../examples/libaxdl/config/scrfd.json) |-|
|ax_bvc|30|```MT_DET_YOLOX_PPL```|[ax_bvc_1024x864](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/ax_bvc_1024x864.joint)|[ax_bvc_det.json](../examples/libaxdl/config/ax_bvc_det.json) |爱芯元智算法组训练的人车非检测模型开源版本|
|yolov8|21|```MT_DET_YOLOV8```|[yolov8n](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov8n.joint)|[ax_bvc_det.json](../examples/libaxdl/config/yolov8.json) |spilt换成slice，后处理分类加了argmax（少个for循环），具体修改方式，请留意交流群以及此页面更新教程文章链接|
|yolov8_seg|26|```MT_DET_YOLOV8_SEG```|[yolov8n-seg](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/yolov8n-seg.joint)|[ax_bvc_det.json](../examples/libaxdl/config/yolov8_seg.json) |同yolov8|
|crowdcound|10|```MT_DET_CROWD_COUNT```|[crowdcount](https://github.com/AXERA-TECH/ax-models/raw/main/ax620/crowdcount.joint)|[crowdcount.json](../examples/libaxdl/config/crowdcount.json) |[密集人群计数](https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet)|